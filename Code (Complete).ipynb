{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03a29530-cdff-4868-b3f0-01a71f560e52",
   "metadata": {},
   "source": [
    "# IMPORTS AND LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5035e7-5a78-421f-9a5d-f1da9f6b6741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# List available GPUs\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Use the second GPU assuming that index '1' is the second GPU\n",
    "        tf.config.experimental.set_visible_devices(gpus[1], 'GPU')\n",
    "        \n",
    "        # Setting GPU memory growth\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c51c92-f7b1-48d2-a4ae-e5a8a0eafc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from joblib import load\n",
    "from keras.applications import DenseNet201\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import GlobalAveragePooling2D, Dense, Dropout, Flatten, Input, Concatenate\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import to_categorical\n",
    "from scipy.signal import convolve2d\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from skimage import io\n",
    "from skimage import measure\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "from skimage.measure import regionprops, label\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.metrics import Metric, Precision, Recall, F1Score\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2ef2d5-1d38-435f-903e-f244d881da4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc42a580-e4dd-4a51-8126-a945e047634b",
   "metadata": {},
   "source": [
    "# PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b25aab-7146-4bb1-83d5-154df4c76592",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26fffe1-0b87-4803-a3aa-270f8658c8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadataTrain = pd.read_csv('data/metadataTrain.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b8cb0d-8e0d-44c4-b96e-4dd9c1b4b276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar Graph of Classes\n",
    "\n",
    "# Count the number of instances of each class\n",
    "classCounts = metadataTrain['CLASS'].value_counts()\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x = classCounts.index, y = classCounts.values, alpha=0.8)\n",
    "plt.title('Class Distribution')\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "plt.xlabel('Class', fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "print('Class Distribution')\n",
    "print(classCounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa602df-2c68-418c-951b-9a1b588e611a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/metadataTrain.csv')\n",
    "\n",
    "# Directory where the new augmented images will be saved\n",
    "augmented_image_dir = 'augmented_images/'\n",
    "\n",
    "# Directory where the original images are stored\n",
    "image_directory = 'data/Train/Train/'\n",
    "\n",
    "# Make sure output directory exists\n",
    "if not os.path.exists(augmented_image_dir):\n",
    "    os.makedirs(augmented_image_dir)\n",
    "\n",
    "augmentation_counts = {\n",
    "        1: 1,   \n",
    "        3: 2,   \n",
    "        4: 11,  \n",
    "        5: 3,   \n",
    "        6: 41,  \n",
    "        7: 40,  \n",
    "        8: 15,  \n",
    "        }\n",
    "\n",
    "# Define your image data generator\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,       # Random rotations from 0 to 20 degrees\n",
    "    width_shift_range=0.1,   # Random horizontal shifts\n",
    "    height_shift_range=0.1,  # Random vertical shifts\n",
    "    shear_range=0.1,         # Shear transformations\n",
    "    zoom_range=0.1,          # Random zoom\n",
    "    horizontal_flip=True,    # Random horizontal flips\n",
    "    vertical_flip=True,      # Random vertical flips\n",
    "    fill_mode='nearest'      # Strategy for filling in new pixels\n",
    ")\n",
    "\n",
    "# Placeholder for new DataFrame rows\n",
    "new_rows = []\n",
    "\n",
    "augmentation_counter = 1  # Start a counter at 1\n",
    "\n",
    "# Now let's modify the existing loop where we perform the augmentation\n",
    "for class_label, num_augmentations in tqdm(augmentation_counts.items()):\n",
    "    # Filter the dataframe for the current class\n",
    "    current_class_df = df[df['CLASS'] == class_label]\n",
    "    \n",
    "    for index, row in current_class_df.iterrows():\n",
    "        # Load the image\n",
    "        image_path = os.path.join(image_directory, row['ID'] + '.jpg')\n",
    "        image = io.imread(image_path)\n",
    "        image = tf.image.convert_image_dtype(image, tf.float32)  # Convert to float32 for tf augmentations\n",
    "        \n",
    "        # Add an extra dimension for batch size, which is expected by ImageDataGenerator\n",
    "        image = np.expand_dims(image, 0)\n",
    "        \n",
    "        # Generate aug_per_function images for each augmentation type\n",
    "        for _ in range(num_augmentations):\n",
    "            # Use .flow() on the single image\n",
    "            for x_batch in datagen.flow(image, batch_size=1):\n",
    "                # Take the first batch and remove the batch dimension\n",
    "                augmented_image = x_batch[0]\n",
    "                \n",
    "                # Convert back to uint8 and remove the extra dimension\n",
    "                augmented_image = tf.image.convert_image_dtype(augmented_image, tf.uint8).numpy()\n",
    "\n",
    "                # Generate a new ID for the augmented image\n",
    "                new_id = f\"{row['ID']}_aug{augmentation_counter}\"\n",
    "                augmentation_counter += 1  # Increment the counter for each new image\n",
    "                \n",
    "                # Save the augmented image\n",
    "                new_image_path = os.path.join(augmented_image_dir, new_id + '.jpg')\n",
    "                io.imsave(new_image_path, augmented_image)\n",
    "\n",
    "                # Add a row to the new_rows list with the new ID and copied metadata\n",
    "                new_row = row.copy()\n",
    "                new_row['ID'] = new_id\n",
    "                new_rows.append(new_row)\n",
    "\n",
    "                # Break after generating one image to move to the next augmentation\n",
    "                break\n",
    "\n",
    "    # Update the DataFrame with the new rows for this class\n",
    "    augmented_df = pd.DataFrame(new_rows)\n",
    "    df = pd.concat([df, augmented_df], ignore_index=True)\n",
    "\n",
    "    # Reset new_rows for the next class\n",
    "    new_rows = []\n",
    "\n",
    "    print(f'Finished augmenting class {class_label}')\n",
    "    \n",
    "# Save the new DataFrame to CSV\n",
    "df.to_csv('augmented_metadata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa352b4-cf21-4d1e-aac1-8fd6e88caf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar Graph of Classes\n",
    "\n",
    "# Count the number of instances of each class\n",
    "classCounts = df['CLASS'].value_counts()\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x = classCounts.index, y = classCounts.values, alpha=0.8)\n",
    "plt.title('Class Distribution')\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "plt.xlabel('Class', fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "print('Class Distribution')\n",
    "print(classCounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905826ad-ac00-4c9a-b972-ccc8893b630a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Image Enhancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540e2bf4-67c1-47e6-b4cd-5853c5d54875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrast_stretching(image):\n",
    "    # Convert to YUV color space\n",
    "    img_yuv = cv2.cvtColor(image, cv2.COLOR_BGR2YUV)\n",
    "    # Apply histogram equalization on the Luminance channel\n",
    "    img_yuv[:,:,0] = cv2.equalizeHist(img_yuv[:,:,0])\n",
    "    # Convert back to BGR color space\n",
    "    image_equalized = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2BGR)\n",
    "    return image_equalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8644d9c7-475a-4301-92c9-22395d669c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your images and where to save the enhanced versions\n",
    "image_directory = 'augmented_images'\n",
    "enhanced_directory = 'enhanced_images'\n",
    "\n",
    "# Create the enhanced images directory if it doesn't exist\n",
    "if not os.path.exists(enhanced_directory):\n",
    "    os.makedirs(enhanced_directory)\n",
    "\n",
    "# Assume 'df' is your DataFrame and it has a column 'ID' with the image filenames\n",
    "for image_id in tqdm(df['ID'], desc='Enhancing images'):\n",
    "    # Build the path to the image file\n",
    "    image_path = os.path.join(image_directory, image_id + '.jpg')\n",
    "    \n",
    "    # Check if the image file exists\n",
    "    if os.path.isfile(image_path):\n",
    "        image = cv2.imread(image_path)\n",
    "    else:\n",
    "        image_path = os.path.join('data/Train/Train', image_id + '.jpg')\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "    # Apply the contrast stretching function\n",
    "    enhanced_image = contrast_stretching(image)\n",
    "    \n",
    "    # Build the path to where the enhanced image will be saved\n",
    "    enhanced_image_path = os.path.join(enhanced_directory, image_id + '_enhanced.jpg')\n",
    "    \n",
    "    # Save the enhanced image\n",
    "    cv2.imwrite(enhanced_image_path, enhanced_image)\n",
    "\n",
    "print(\"Enhancement process completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4771363-775a-4f02-91b6-c2d0183ccd5a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Segmentation Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a07b2e-943f-487c-b425-f799ad7e4438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directory with images and mask directory\n",
    "data_dir = 'enhanced_images'\n",
    "mask_dir = 'data/Train/Masks/'\n",
    "\n",
    "# Ensure the mask directory exists\n",
    "if not os.path.exists(mask_dir):\n",
    "    os.makedirs(mask_dir)\n",
    "\n",
    "# List all jpg images in the directory\n",
    "image_files = [f for f in os.listdir(data_dir) if f.endswith('.jpg')]\n",
    "\n",
    "for image_file in tqdm(image_files):\n",
    "    # Construct the full path to the image\n",
    "    file_path = os.path.join(data_dir, image_file)\n",
    "    \n",
    "    # Construct the full path for the new mask\n",
    "    mask_path = os.path.join(mask_dir, image_file.replace('.jpg', '_seg.png'))\n",
    "\n",
    "    org_mask = os.path.join('data/Train/', image_file.replace('.jpg', '_seg.png'))\n",
    "    \n",
    "    # If the mask already exists, skip to the next one\n",
    "    if os.path.exists(org_mask):\n",
    "        continue\n",
    "\n",
    "    # Load the image in grayscale\n",
    "    image = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # Apply Gaussian blur to reduce noise\n",
    "    blurred_image = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "    \n",
    "    # Apply Otsu's thresholding\n",
    "    ret, otsu_mask = cv2.threshold(blurred_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Invert mask if required: lesions are white, background is black\n",
    "    otsu_mask = 255 - otsu_mask\n",
    "    \n",
    "    # Save the mask to mask directory\n",
    "    cv2.imwrite(mask_path, otsu_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240ef023-47c4-4d39-8d80-c61490e3047b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# ABCD Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57bdeae-f6fe-4165-b253-633e272e689c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_area(mask):\n",
    "    # Assuming the mask is a binary image with 1s for the lesion and 0s for the background\n",
    "    area = np.sum(mask == 255)  # Count the number of pixels that are white\n",
    "    return area    \n",
    "\n",
    "def calculate_perimeter(mask):\n",
    "    # Find contours using OpenCV\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    # Assuming that the largest contour corresponds to the lesion\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "    # Calculate the perimeter of the largest contour\n",
    "    perimeter = cv2.arcLength(largest_contour, True)\n",
    "    return perimeter\n",
    "\n",
    "def calculate_circularity(area, perimeter):\n",
    "    if perimeter == 0:\n",
    "        return 0  # To avoid division by zero\n",
    "    circularity = (4 * np.pi * area) / (perimeter ** 2)\n",
    "    return circularity\n",
    "\n",
    "def calculate_bulkiness(mask):\n",
    "    # Find contours as done for the perimeter calculation\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Assuming that the largest contour corresponds to the lesion\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "    \n",
    "    # Calculate the convex hull of the largest contour\n",
    "    hull = cv2.convexHull(largest_contour)\n",
    "    \n",
    "    # Calculate the area of the convex hull\n",
    "    hull_area = cv2.contourArea(hull)\n",
    "    \n",
    "    # Calculate the area of the lesion\n",
    "    lesion_area = cv2.contourArea(largest_contour)\n",
    "    \n",
    "    if hull_area == 0:\n",
    "        return 0  # To avoid division by zero\n",
    "    \n",
    "    # Bulkiness is the ratio of the lesion's area to its convex hull's area\n",
    "    bulkiness = lesion_area / hull_area\n",
    "    return bulkiness\n",
    "\n",
    "def calculate_solidity(mask):\n",
    "    # Find contours using OpenCV\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Assuming the largest contour corresponds to the lesion\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "    \n",
    "    # Calculate the convex hull of the largest contour\n",
    "    hull = cv2.convexHull(largest_contour)\n",
    "    \n",
    "    # Calculate the area of the largest contour (lesion area)\n",
    "    lesion_area = cv2.contourArea(largest_contour)\n",
    "    \n",
    "    # Calculate the area of the convex hull\n",
    "    hull_area = cv2.contourArea(hull)\n",
    "    \n",
    "    if hull_area == 0:\n",
    "        return 0  # To prevent division by zero\n",
    "    \n",
    "    # Solidity is the ratio of the lesion's area to its convex hull's area\n",
    "    solidity = lesion_area / hull_area\n",
    "    return solidity\n",
    "\n",
    "def calculate_eccentricity(mask):\n",
    "    # Find contours using OpenCV\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    # Assuming the largest contour corresponds to the lesion\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "    # Calculate image moments of the largest contour\n",
    "    moments = cv2.moments(largest_contour)\n",
    "    \n",
    "    # Calculate x and y coordinates of the center of the lesion\n",
    "    # Avoid division by zero in case the moment \"m00\" is zero\n",
    "    if moments['m00'] == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Central moments\n",
    "    cx = int(moments['m10']/moments['m00'])\n",
    "    cy = int(moments['m01']/moments['m00'])\n",
    "\n",
    "    # Calculate the central second moments (mu20, mu02, mu11)\n",
    "    mu20 = moments['mu20'] / moments['m00']\n",
    "    mu02 = moments['mu02'] / moments['m00']\n",
    "    mu11 = moments['mu11'] / moments['m00']\n",
    "    \n",
    "    # Calculate the eccentricity of the ellipse\n",
    "    # Eccentricity formula: sqrt(1 - (minor_axis^2 / major_axis^2))\n",
    "    # Where:\n",
    "    # minor_axis^2 = (mu20 + mu02 - sqrt((mu20 - mu02)**2 + 4*mu11**2)) / 2\n",
    "    # major_axis^2 = (mu20 + mu02 + sqrt((mu20 - mu02)**2 + 4*mu11**2)) / 2\n",
    "    common = np.sqrt((mu20 - mu02) ** 2 + 4 * mu11 ** 2)\n",
    "    major_axis_squared = (mu20 + mu02 + common) / 2\n",
    "    minor_axis_squared = (mu20 + mu02 - common) / 2\n",
    "\n",
    "    # Avoid division by zero\n",
    "    if major_axis_squared == 0:\n",
    "        return 0\n",
    "\n",
    "    eccentricity = np.sqrt(1 - (minor_axis_squared / major_axis_squared))\n",
    "    return eccentricity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3fead7-ce16-4407-a25f-1cbc3cf2e821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_edges(mask):\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    edge_mask = np.zeros_like(mask)\n",
    "    cv2.drawContours(edge_mask, contours, -1, (255), thickness=1)\n",
    "    return edge_mask\n",
    "\n",
    "def calculate_gradients(channel_image, edge_mask):\n",
    "    # Compute gradients along the x and y axis directly on the single-channel image\n",
    "    grad_x = cv2.Sobel(channel_image, cv2.CV_64F, 1, 0, ksize=5)\n",
    "    grad_y = cv2.Sobel(channel_image, cv2.CV_64F, 0, 1, ksize=5)\n",
    "    \n",
    "    # Compute gradient magnitude\n",
    "    grad_magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
    "    \n",
    "    # Mask the gradient image to only include the edges\n",
    "    edge_gradients = grad_magnitude[edge_mask == 255]\n",
    "    return edge_gradients\n",
    "\n",
    "def border_gradient_stats(image, mask):\n",
    "    edge_mask = find_edges(mask)\n",
    "    stats = {}\n",
    "    channels = ['Red', 'Green', 'Blue']\n",
    "    for i, color in enumerate(channels):\n",
    "        # Directly use the individual color channel\n",
    "        channel_image = image[:, :, i]\n",
    "        \n",
    "        # Calculate gradients on the specific channel\n",
    "        channel_gradients = calculate_gradients(channel_image, edge_mask)\n",
    "        \n",
    "        # Calculate mean and standard deviation and update dictionary with specific keys\n",
    "        mean = np.mean(channel_gradients)\n",
    "        std = np.std(channel_gradients)\n",
    "        \n",
    "        stats[f'mean_{color}'] = mean\n",
    "        stats[f'std_{color}'] = std\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c24b43-4ea9-4625-b1fc-785e87734e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def channel_stats(image, mask):\n",
    "    # Check if the image is loaded in RGB format or convert if necessary\n",
    "    if image.shape[2] == 3:  # Assuming image has three channels\n",
    "        # Assuming image is BGR (common with cv2.imread), convert to RGB\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    stats = {}\n",
    "    channels = ['Red', 'Green', 'Blue']\n",
    "    \n",
    "    # Ensure the mask is boolean\n",
    "    mask_boolean = mask > 0\n",
    "    \n",
    "    for i, color in enumerate(channels):\n",
    "        # Extract the channel\n",
    "        channel = image[:, :, i]\n",
    "        # Apply the mask to the channel\n",
    "        masked_channel = channel[mask_boolean]\n",
    "        \n",
    "        if masked_channel.size == 0:\n",
    "            mean = std = np.nan  # Handle case where mask is empty\n",
    "        else:\n",
    "            # Calculate mean and standard deviation\n",
    "            mean = np.mean(masked_channel)\n",
    "            std = np.std(masked_channel)\n",
    "        \n",
    "        stats[f'mean_{color}'] = mean\n",
    "        stats[f'std_{color}'] = std\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecdbdd8-9f0d-468b-8b5f-20f553985a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_texture_features(image, mask, distances, angles):\n",
    "    # Convert the image to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Mask the grayscale image to focus on the lesion\n",
    "    masked_image = np.where(mask > 0, gray_image, 0)\n",
    "    \n",
    "    # Calculate the GLCM\n",
    "    glcm = graycomatrix(masked_image, distances, angles, 256, symmetric=True, normed=True)\n",
    "    \n",
    "    # List of properties to calculate\n",
    "    properties = ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation']\n",
    "    texture_features = {}\n",
    "    \n",
    "    # Calculate texture properties for each property and angle\n",
    "    for prop in properties:\n",
    "        for i, angle in enumerate(angles):\n",
    "            angle_deg = np.degrees(angle)  # Convert radians to degrees for labeling\n",
    "            feature_name = f'{prop}_{int(angle_deg)}deg'\n",
    "            # Extract the feature for each angle and store it under a unique key\n",
    "            texture_features[feature_name] = graycoprops(glcm, prop)[0, i]\n",
    "    \n",
    "    return texture_features\n",
    "\n",
    "def weber(grayscale_image):\n",
    "    grayscale_image = grayscale_image.astype(np.float64)\n",
    "    grayscale_image[grayscale_image==0] = np.finfo(float).eps\n",
    "    neighbours_filter = np.array([\n",
    "        [1,1,1],\n",
    "        [1,0,1],\n",
    "        [1,1,1]\n",
    "    ])\n",
    "    convolved = convolve2d(grayscale_image,neighbours_filter, mode='same')\n",
    "    weber_descriptor = convolved-8*grayscale_image\n",
    "    weber_descriptor = weber_descriptor/grayscale_image\n",
    "    weber_descriptor = np.arctan(weber_descriptor)\n",
    "    return weber_descriptor\n",
    "\n",
    "def calculate_wld_features(image, mask):\n",
    "    # Convert the image to grayscale if it isn't already\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if len(image.shape) == 3 else image\n",
    "    \n",
    "    # Compute the WLD for the whole image\n",
    "    wld_image = weber(gray_image)\n",
    "    \n",
    "    # Apply mask to the WLD image to focus only on the lesion\n",
    "    lesion_wld = wld_image[mask > 0]\n",
    "\n",
    "    # Compute mean and standard deviation of the WLD values within the lesion\n",
    "    mean_wld = np.mean(lesion_wld)\n",
    "    std_wld = np.std(lesion_wld)\n",
    "    \n",
    "    return mean_wld, std_wld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c70996-3c3f-43f5-ab6a-a8c0a5fc7286",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadataTest = pd.read_csv('augmented_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f7563f-2527-479d-8db2-8b60e15bc7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_dir = 'data/Train/Masks/'\n",
    "seg_files = [os.path.splitext(f)[0] for f in os.listdir(segmented_dir)]\n",
    "seg_files = [filename.replace('_enhanced_seg', '') for filename in seg_files]\n",
    "len(seg_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e10b04c-e5a4-4c1e-a7a5-d186274ca9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_metadata = metadataTest[metadataTest['ID'].isin(seg_files)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79edf13-89eb-4cf3-972a-bf33a2e57071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store features\n",
    "features_list = []\n",
    "data_dir = 'enhanced_images'\n",
    "segmented_dir = 'data/Train/Masks/'\n",
    "\n",
    "# Retrieve list of files and filter out non-image files\n",
    "image_files = [f for f in os.listdir(data_dir) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "\n",
    "# Process each segmented image with tqdm wrapper for the progress bar\n",
    "for f in tqdm(image_files, desc='Processing images'):\n",
    "    image_path = os.path.join(data_dir, f)\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Correcting mask path creation\n",
    "    mask_path = os.path.join(segmented_dir, f.replace('.jpg', '_seg.png'))\n",
    "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "    mask_image_binary = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)[1]\n",
    "\n",
    "    # print(image_path)\n",
    "    # print(mask_path)\n",
    "\n",
    "    # # Plot original and enhanced images side by side\n",
    "    # plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # # Display original image\n",
    "    # plt.subplot(1, 2, 1)\n",
    "    # plt.imshow(image)\n",
    "    # plt.title('Image')\n",
    "    # plt.axis('off')\n",
    "\n",
    "    # # Display enhanced image\n",
    "    # plt.subplot(1, 2, 2)\n",
    "    # plt.imshow(mask)\n",
    "    # plt.title('Mask')\n",
    "    # plt.axis('off')\n",
    "\n",
    "    # plt.show()\n",
    "\n",
    "    # Feature calculations\n",
    "    lesion_area = calculate_area(mask_image_binary)\n",
    "    lesion_perimeter = calculate_perimeter(mask_image_binary)\n",
    "    lesion_circularity = calculate_circularity(lesion_area, lesion_perimeter)\n",
    "    lesion_bulkiness = calculate_bulkiness(mask_image_binary)\n",
    "    lesion_solidity = calculate_solidity(mask_image_binary)\n",
    "    lesion_eccentricity = calculate_eccentricity(mask_image_binary)\n",
    "    gradient_stats = border_gradient_stats(image, mask_image_binary)\n",
    "    channel_statistics = channel_stats(image, mask_image_binary)\n",
    "    distances = [1]\n",
    "    angles = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "    texture_features = calculate_texture_features(image, mask_image_binary, distances, angles)\n",
    "    mean_wld, std_wld = calculate_wld_features(image, mask_image_binary)\n",
    "\n",
    "    # Build a single dictionary entry for this image\n",
    "    feature_dict = {\n",
    "        'ID': f.replace('_enhanced.jpg', ''),\n",
    "        'Area': lesion_area,\n",
    "        'Perimeter': lesion_perimeter,\n",
    "        'Circularity': lesion_circularity,\n",
    "        'Bulkiness': lesion_bulkiness,\n",
    "        'Solidity': lesion_solidity,\n",
    "        'Eccentricity': lesion_eccentricity,\n",
    "        'WLD_Mean': mean_wld,\n",
    "        'WLD_Std': std_wld,\n",
    "    }\n",
    "\n",
    "    # Merge dictionaries from gradient, color, and texture stats\n",
    "    feature_dict.update({f'B_{key}': value for key, value in gradient_stats.items()})\n",
    "    feature_dict.update({f'C_{key}': value for key, value in channel_statistics.items()})\n",
    "    feature_dict.update({f'D_{key}': value for key, value in texture_features.items()})\n",
    "\n",
    "    # Append to list\n",
    "    features_list.append(feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28800003-7099-4d6f-80ca-9c254ff3ffa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of features to a DataFrame\n",
    "features_df = pd.DataFrame(features_list)\n",
    "\n",
    "features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c32de36-c3fb-4f20-9412-88b26f3ae472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "features_df.to_csv('lesion_features_V2.1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84b484c-8916-4b93-8741-251e455c0a91",
   "metadata": {},
   "source": [
    "# MERGING METADATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ab2ef3-0d5c-4324-b9ce-f95205d3457f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_augmented = pd.read_csv('augmented_metadata.csv') \n",
    "df_abcd = pd.read_csv('lesion_features_V2.1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a6d6dd-619c-4e4b-8d76-e7b5625b1c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(df_abcd, df_augmented, on='ID', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51591529-3e43-48cd-b535-7071d05aba74",
   "metadata": {},
   "source": [
    "# IMAGE AQUISITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d1efd6-a8ee-46b1-b52b-639e21385dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "\n",
    "enhanced_image_dir = 'enhanced_images'\n",
    "target_size = (150, 150)\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Loop through the DataFrame and process each image\n",
    "for index, row in tqdm(merged_df.iterrows(), total = merged_df.shape[0], desc = \"Processing images\"):\n",
    "    image_id = row['ID']\n",
    "    image_class = row['CLASS']\n",
    "\n",
    "    # Construct the path \n",
    "    image_path = os.path.join(enhanced_image_dir, image_id + '_enhanced.jpg')  \n",
    "\n",
    "    # Check if the image file exists\n",
    "    if os.path.exists(image_path):\n",
    "        # Load the image from file\n",
    "        img = image.load_img(image_path, target_size=target_size)\n",
    "        img_tensor = image.img_to_array(img)\n",
    "        img_tensor /= 255.  # Normalize the image\n",
    "\n",
    "        images.append(img_tensor)\n",
    "        labels.append(image_class)\n",
    "    else:\n",
    "        print(f\"Image not found: {image_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49678876-694f-469f-bb8d-04f4f3e37315",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_array = np.array(images)\n",
    "labels_array = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a996fa76-a56e-43ee-b4cc-ac76516f4d95",
   "metadata": {},
   "source": [
    "# DATA SET SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca19be1-a12c-4b61-b65a-46e5fcd8875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_array = images_array[0:30000]\n",
    "labels_array = labels_array[0:30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d18d72-e993-402d-8925-2750c4a0fcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(len(images_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08f334e-f4de-4466-bc21-ec731296e78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split indices into training and temporary (validation + test) sets\n",
    "train_indices, temp_indices, train_labels, temp_labels = train_test_split(\n",
    "    indices, labels_array, test_size=0.3, random_state=42, stratify=labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17516f3-be1a-49a9-ae10-a9b9239842cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use indices to create the actual data splits\n",
    "train_images = images_array[train_indices]\n",
    "train_features = merged_df.iloc[train_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81e912a-6cca-4b1b-b26b-144094ca640c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split temporary set further into validation and test sets\n",
    "val_indices, test_indices, val_labels, test_labels = train_test_split(\n",
    "    temp_indices, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels)\n",
    "\n",
    "val_images = images_array[val_indices]\n",
    "val_features = merged_df.iloc[val_indices]\n",
    "\n",
    "test_images = images_array[test_indices]\n",
    "test_features = merged_df.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd43c5b-3333-40c3-b58f-f3cf2ff700c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = np.array(train_images)\n",
    "val_images = np.array(val_images)\n",
    "test_images = np.array(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a041dbf-3444-4389-8f53-ef9d3c25267e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract 1 from each label as they are from 1 to 8\n",
    "train_labels = train_labels - 1\n",
    "val_labels = val_labels - 1\n",
    "test_labels = test_labels - 1\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "train_labels = to_categorical(train_labels, num_classes=8)\n",
    "val_labels = to_categorical(val_labels, num_classes=8)\n",
    "test_labels = to_categorical(test_labels, num_classes=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcbab66-e9cf-4934-87d5-69be78cadb06",
   "metadata": {},
   "source": [
    "# DATAFRAME COMPILATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad815963-8e57-407c-adfe-9838494be811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_class_average(row):\n",
    "    if pd.isna(row['AGE']):\n",
    "        return average_ages_per_class[row['CLASS']]\n",
    "    else:\n",
    "        return row['AGE']\n",
    "\n",
    "def replace_with_common_position(row):\n",
    "    if pd.isna(row['POSITION']):\n",
    "        return most_common_positions[row['CLASS']]\n",
    "    else:\n",
    "        return row['POSITION']\n",
    "\n",
    "def replace_with_common_gender(row):\n",
    "    if pd.isna(row['SEX']):\n",
    "        return most_common_gender[row['CLASS']]\n",
    "    else:\n",
    "        return row['SEX']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b390db-751c-4923-87ea-d790863c8ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_ages_per_class = merged_df.groupby('CLASS')['AGE'].mean()\n",
    "train_features.loc[:, 'AGE'] = train_features.apply(replace_with_class_average, axis=1)\n",
    "test_features.loc[:, 'AGE'] = test_features.apply(replace_with_class_average, axis=1)\n",
    "val_features.loc[:, 'AGE'] = val_features.apply(replace_with_class_average, axis=1)\n",
    "\n",
    "most_common_positions = merged_df.groupby('CLASS')['POSITION'].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)\n",
    "train_features.loc[:, 'POSITION'] = train_features.apply(replace_with_common_position, axis=1)\n",
    "test_features.loc[:, 'POSITION'] = test_features.apply(replace_with_common_position, axis=1)\n",
    "val_features.loc[:, 'POSITION'] = val_features.apply(replace_with_common_position, axis=1)\n",
    "\n",
    "most_common_gender = merged_df.groupby('CLASS')['SEX'].agg(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)\n",
    "train_features.loc[:, 'SEX'] = train_features.apply(replace_with_common_gender, axis=1)\n",
    "test_features.loc[:, 'SEX'] = test_features.apply(replace_with_common_gender, axis=1)\n",
    "val_features.loc[:, 'SEX'] = val_features.apply(replace_with_common_gender, axis=1)\n",
    "\n",
    "train_features.loc[:, 'AGE'] = (train_features['AGE'] - train_features['AGE'].mean()) / train_features['AGE'].std()\n",
    "test_features.loc[:, 'AGE'] = (test_features['AGE'] - test_features['AGE'].mean()) / test_features['AGE'].std()\n",
    "val_features.loc[:, 'AGE'] = (val_features['AGE'] - val_features['AGE'].mean()) / val_features['AGE'].std()\n",
    "\n",
    "train_features = pd.get_dummies(train_features, columns=['SEX'], drop_first=True)\n",
    "test_features = pd.get_dummies(test_features, columns=['SEX'], drop_first=True)\n",
    "val_features = pd.get_dummies(val_features, columns=['SEX'], drop_first=True)\n",
    "\n",
    "train_features = pd.get_dummies(train_features, columns=['POSITION'], drop_first=True)\n",
    "test_features = pd.get_dummies(test_features, columns=['POSITION'], drop_first=True)\n",
    "val_features = pd.get_dummies(val_features, columns=['POSITION'], drop_first=True)\n",
    "\n",
    "train_features = train_features.drop('CLASS', axis=1)\n",
    "test_features = test_features.drop('CLASS', axis=1)\n",
    "val_features = val_features.drop('CLASS', axis=1)\n",
    "\n",
    "train_features = train_features.drop('ID', axis=1)\n",
    "test_features = test_features.drop('ID', axis=1)\n",
    "val_features = val_features.drop('ID', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40c3d71-2c6b-49d2-a26a-bfed1b4272bf",
   "metadata": {},
   "source": [
    "# INITIAL MODEL SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaf7829-7829-4400-8f19-a5695f614c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns1 = [col for col in merged_df.columns if col.endswith('45deg')]\n",
    "columns2 = [col for col in merged_df.columns if col.endswith('90deg')]\n",
    "columns3 = [col for col in merged_df.columns if col.endswith('135deg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c9797b-7c7c-42fe-9272-ee8c20b25a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_m1 = train_features\n",
    "val_features_m1 = val_features\n",
    "test_features_m1 = test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c35304-9255-4386-a975-656a5eaa77f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_m1 = train_features_m1.drop(columns=columns1)\n",
    "train_features_m1 = train_features_m1.drop(columns=columns2)\n",
    "train_features_m1 = train_features_m1.drop(columns=columns3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c649d2a-f9c6-4f62-b583-5449734e0e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_features_m1 = val_features_m1.drop(columns=columns1)\n",
    "val_features_m1 = val_features_m1.drop(columns=columns2)\n",
    "val_features_m1 = val_features_m1.drop(columns=columns3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74566afb-740b-49b4-b0a8-fbb7bbae3a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_m1 = test_features_m1.drop(columns=columns1)\n",
    "test_features_m1 = test_features_m1.drop(columns=columns2)\n",
    "test_features_m1 = test_features_m1.drop(columns=columns3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874cf000-25ef-4b57-83f8-e8597edb659f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_m1 = train_features_m1.astype(np.float32)\n",
    "val_features_m1 = val_features_m1.astype(np.float32)\n",
    "test_features_m1 = test_features_m1.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9212a98a-954e-4479-98b6-4219a5bfe1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_m1['POSITION_anterior torso'] = 0\n",
    "val_features_m1['POSITION_anterior torso'] = 0\n",
    "test_features_m1['POSITION_anterior torso'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135266f3-ee34-4cdc-8d19-ab68b3882b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "\n",
    "image_dir = 'augmented_images'\n",
    "target_size = (150, 150)\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Loop through the DataFrame and process each image\n",
    "for index, row in tqdm(merged_df.iterrows(), total = merged_df.shape[0], desc = \"Processing images\"):\n",
    "    image_id = row['ID']\n",
    "    image_class = row['CLASS']\n",
    "\n",
    "    # Construct the path \n",
    "    image_path = os.path.join(image_dir, image_id + '.jpg')  \n",
    "\n",
    "    try:\n",
    "        img = image.load_img(image_path, target_size=target_size)\n",
    "    except FileNotFoundError:\n",
    "        path = os.path.join('data/Train/Train', image_id + '.jpg')\n",
    "        img = image.load_img(path, target_size=target_size)\n",
    "        \n",
    "    img_tensor = image.img_to_array(img)\n",
    "    img_tensor /= 255.  # Normalize the image\n",
    "\n",
    "    images.append(img_tensor)\n",
    "    labels.append(image_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9555977a-bf4b-4ddd-9b21-59ade069bbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_array = np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06012a49-9aab-4c90-a062-02f70efd07b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_m1 = images_array[train_indices]\n",
    "val_images_m1 = images_array[val_indices]\n",
    "test_images_m1 = images_array[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91788cdc-3d29-42fb-9589-9a21441f76c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DenseNet201 with pre-trained weights, without the top layer\n",
    "base_model = DenseNet201(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f84546-21cc-4053-8cbf-1c79fd8c8ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For image input\n",
    "image_input = base_model.input\n",
    "image_features = Flatten()(base_model.output)  # Flatten the output\n",
    "\n",
    "image_features = base_model.output\n",
    "image_features = GlobalAveragePooling2D()(image_features)  # This will add the pooling layer\n",
    "\n",
    "# Now, the image_features should have a defined shape\n",
    "print('Image features shape:', image_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c8af5d-47f1-4f7f-bcb0-4c09ed3f3339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of metadata features\n",
    "num_metadata_features = train_features.shape[1]\n",
    "\n",
    "# For metadata input \n",
    "metadata_input = Input(shape=(num_metadata_features,))\n",
    "\n",
    "# Concatenate image features and metadata\n",
    "combined_features = Concatenate()([image_features, metadata_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71366ae0-0bbb-49b1-a49c-7878aa77eda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a dense layer for further processing\n",
    "x = Dense(256, activation='relu')(combined_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7143ccaf-3bcc-413a-aba6-c62d6baf4e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the model from the file\n",
    "# model = load_model('Model_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542b6480-66c6-4307-a040-1f7229fec199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output layer for classification\n",
    "output = Dense(8, activation='softmax')(x)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=[image_input, metadata_input], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8fedfe-a92f-4efc-8142-9e930e1a5572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "optimizer = Adam(learning_rate = 1e-5)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', Precision(), Recall(), F1Score()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d43c8f5-a3ae-4277-837e-f228f5725215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, verbose=1, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff7da07-2e6f-42ae-a184-9f6d19652bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([train_images_m1, train_features],\n",
    "          train_labels,\n",
    "          validation_data=([val_images_m1, val_features], val_labels),\n",
    "          epochs=50,  \n",
    "          callbacks=[early_stopping],\n",
    "          verbose=1\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02451707-3261-4b5c-8181-5d9d8b11e6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate([test_images_m1, test_features_m1], test_labels)\n",
    "print(f\"Test Loss: {results[0]}\")\n",
    "print(f\"Test Accuracy: {results[1]}\")\n",
    "print(f\"Test Precision: {results[2]}\")\n",
    "print(f\"Test Recall: {results[3]}\")\n",
    "print(f\"Test F1-Score: {results[4]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8432809-c1fc-4a6a-9939-8d876614c9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('Model_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8297f3-fe2b-4abf-8b75-c7791250eca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4938eb-e31a-4a50-a687-1e57124cd209",
   "metadata": {},
   "source": [
    "# HYBRID MODEL IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c206fcc1-fa4b-4dc7-b310-ad5f6199f39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_features = scaler.fit_transform(train_features)\n",
    "test_features = scaler.fit_transform(test_features)\n",
    "val_features = scaler.fit_transform(val_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5b0b68-35be-4226-a536-836291716ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DenseNet201 model pre-trained on ImageNet data\n",
    "base_model = DenseNet201(weights='imagenet', include_top=False)\n",
    "number_of_classes = 8\n",
    "number_of_epochs = 50\n",
    "# batch_size = 16\n",
    "further_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c29842-b495-49d3-9313-27f1b27b0c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the layers of the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom layers on top of the base model\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)  # Regularize with dropout\n",
    "predictions = Dense(number_of_classes, activation='softmax')(x)  # Replace with your number of classes\n",
    "\n",
    "# Create the final model\n",
    "model = Model(inputs = base_model.input, outputs = predictions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer = Adam(learning_rate = 0.0001), loss = 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b2b488-874a-4726-9ccd-758a60e7b15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_pre = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278dca71-da3c-4038-84b5-ac69129cbd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on your data\n",
    "model.fit(train_images, \n",
    "          train_labels, \n",
    "          validation_data = (val_images, val_labels),\n",
    "          epochs = number_of_epochs,\n",
    "          callbacks=[early_stopping_pre],\n",
    "          verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b688e7-d5d9-4520-9a9e-a73908d8c097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Optionally fine-tune some layers\n",
    "for layer in base_model.layers[:-50]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Re-compile the model after unfreezing\n",
    "model.compile(optimizer = Adam(learning_rate = 0.00001), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Setup early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, verbose=1, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b591ec1-20a0-43e1-b887-7ad5dfff23d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue training\n",
    "model.fit(train_images, \n",
    "          train_labels, \n",
    "          validation_data = (val_images, val_labels),\n",
    "          epochs = further_epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          verbose=1\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3f1bd7-4a42-4539-97d5-48fb163e6058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the model from the file\n",
    "# model = load_model('Model_DenseNet_v2(Complete)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443f7d84-cec5-406d-b2b7-3af44c5e5ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class probabilities using DenseNet201\n",
    "densenet_val_predictions = model.predict(val_images)  \n",
    "densenet_test_predictions = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8d5122-c43c-4564-a8e7-f50b2634d2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.argmax(train_labels, axis=1)\n",
    "val_labels = np.argmax(val_labels, axis=1)\n",
    "test_labels = np.argmax(test_labels, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be86e8c9-2d3a-4a49-a634-2666d7a46601",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_labels + 1\n",
    "val_labels = val_labels + 1\n",
    "test_labels = test_labels + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac5528c-ff94-401b-afa6-82a46e273516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a pipeline combining a standard scaler and the SVC\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC(probability=True, random_state=42))\n",
    "])\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'svm__C': [0.1, 1, 10],\n",
    "    'svm__kernel': ['rbf', 'linear']\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV object\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', verbose = 2)\n",
    "\n",
    "# Fit grid search on the training data\n",
    "grid_search.fit(train_features, train_labels)\n",
    "\n",
    "# Print best parameters and best score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7fdb56-a728-44a5-a8b4-a14f615aad88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from file\n",
    "grid_search = load('best_model.joblib')\n",
    "\n",
    "# Load the pipeline\n",
    "pipeline = load('best_model.joblib')\n",
    "\n",
    "# Print the steps of the pipeline to see what it contains\n",
    "print(\"Pipeline steps:\", pipeline.steps)\n",
    "\n",
    "# To see the configuration of a specific step, for example, the SVM step if it exists\n",
    "if 'svm' in dict(pipeline.steps):\n",
    "    svm_step = pipeline.named_steps['svm']\n",
    "    print(\"SVM configuration:\", svm_step.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9985754-057b-4a7a-af02-d9b3219f7e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class probabilities on the test set\n",
    "svm_test_predictions = grid_search.predict_proba(test_features)\n",
    "svm_val_predictions = grid_search.predict_proba(val_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030561e5-056e-420e-880f-f2f5a49e7915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack the predictions alongside each other\n",
    "stacked_val_features = np.hstack((densenet_val_predictions, svm_val_predictions))\n",
    "stacked_test_features = np.hstack((densenet_test_predictions, svm_test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb876547-9fc3-429d-9d5c-e5e488af8a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize the stacking model\n",
    "stacking_model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Train the stacking model on the validation predictions\n",
    "# 'val_labels' should be the true labels of the validation set\n",
    "stacking_model.fit(stacked_val_features, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282b4e7d-c5c0-43d3-a18a-f4fc867c6ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Use the stacking model to make predictions on the test set\n",
    "stacked_test_predictions = stacking_model.predict(stacked_test_features)\n",
    "\n",
    "# Evaluate the stacked model\n",
    "stacked_accuracy = accuracy_score(test_labels, stacked_test_predictions)\n",
    "precision = precision_score(test_labels, stacked_test_predictions, average='macro')\n",
    "recall = recall_score(test_labels, stacked_test_predictions, average='macro')\n",
    "f1 = f1_score(test_labels, stacked_test_predictions, average='macro')\n",
    "\n",
    "print(f'Stacked Model Accuracy: {stacked_accuracy}')\n",
    "print(f'Stacked Model Precision (Macro-Averaged): {precision}')\n",
    "print(f'Stacked Model Recall (Macro-Averaged): {recall}')\n",
    "print(f'Stacked Model F1-Score (Macro-Averaged): {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69778826-a1f9-4ef7-afb5-1a43d28f85f3",
   "metadata": {},
   "source": [
    "# FINAL MODEL OPTIMIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6381bef-3049-4bc0-a4b1-0218de802900",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c47e9ff-a3f1-4127-8d04-3b19025e0518",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_labels - 1\n",
    "val_labels = val_labels - 1\n",
    "test_labels = test_labels - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c332bfaf-65df-4d31-9362-e9147aa52faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = to_categorical(train_labels, num_classes=8)\n",
    "val_labels = to_categorical(val_labels, num_classes=8)\n",
    "test_labels = to_categorical(test_labels, num_classes=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6450ff02-64a5-4ead-92fd-203ad358da1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DenseNet201 model pre-trained on ImageNet data\n",
    "base_model = DenseNet201(weights = 'imagenet', include_top = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9c67c4-173a-4248-85a4-209194f14a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_classes = 8\n",
    "number_of_epochs = 50\n",
    "# batch_size = 16\n",
    "further_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1409076a-66be-4a57-86fb-6130975ccee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the layers of the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom layers on top of the base model\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)  # Regularize with dropout\n",
    "predictions = Dense(number_of_classes, activation='softmax')(x)  # Replace with your number of classes\n",
    "\n",
    "# Create the final model\n",
    "model = Model(inputs = base_model.input, outputs = predictions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer = Adam(learning_rate = 0.0001), loss = 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8083981e-a7b7-480e-bfa0-8de7505efaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_pre = EarlyStopping(monitor = 'val_loss', patience = 10, verbose = 1, restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11259a86-b6e9-4acc-a49a-66c1e07e593e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create weights for the training set: higher for misclassified cases\n",
    "sample_weights = np.ones(shape=(len(train_labels),))\n",
    "sample_weights[misclassified_indices] = 10  # Increase the weight for misclassified cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae14e83-5eb3-42b6-889a-e48cee9226d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DenseNet with sample weights\n",
    "model.fit(train_images, \n",
    "          train_labels, \n",
    "          validation_data = (val_images, val_labels),\n",
    "          sample_weight = sample_weights, \n",
    "          epochs = number_of_epochs, \n",
    "          callbacks = [early_stopping_pre],\n",
    "          verbose = 1\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd61cf2c-64d9-43d5-8943-7949b0cce093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the mixed precision policy to 'mixed_float16'\n",
    "set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4eb77f-79ad-4442-9f48-9ed729c1e84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally fine-tune some layers\n",
    "for layer in model.layers[:-50]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Re-compile the model after unfreezing\n",
    "model.compile(optimizer = Adam(learning_rate = 0.00001), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Setup early stopping\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss', patience = 15, verbose = 1, restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c29b1ff-afe7-4c68-ad9a-bf7b805c72db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue training\n",
    "model.fit(train_images, \n",
    "          train_labels, \n",
    "          validation_data = (val_images, val_labels),\n",
    "          sample_weight = sample_weights,\n",
    "          epochs = further_epochs,\n",
    "          callbacks = [early_stopping],\n",
    "          verbose = 1\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d578b8-cd51-4f65-8f91-9cf8a22ff084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the model from the file\n",
    "# model = load_model('Model_DenseNet_v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fb65e3-c0d5-48cb-ae52-d57b681ea0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class probabilities using DenseNet201\n",
    "densenet_train_predictions = model.predict(train_images)\n",
    "densenet_val_predictions = model.predict(val_images)\n",
    "densenet_test_predictions = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ba7e67-0a88-48e5-8f11-04b78323984f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_labels.astype('float32')\n",
    "val_labels = val_labels.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce68ab1-a71b-47d4-8f37-152269539071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine predictions and apply PCA\n",
    "train_combined_features = np.hstack((svm_train_predictions, densenet_train_predictions))\n",
    "pca = PCA(n_components = 0.95)  # Retains 95% of variance\n",
    "train_reduced_features = pca.fit_transform(train_combined_features)\n",
    "\n",
    "val_combined_features = np.hstack((svm_val_predictions, densenet_val_predictions))  \n",
    "val_reduced_features = pca.transform(val_combined_features)\n",
    "\n",
    "train_reduced_features = train_reduced_features.astype('float32')\n",
    "val_reduced_features = val_reduced_features.astype('float32')\n",
    "\n",
    "# Define the neural network\n",
    "model_final = Sequential([\n",
    "    Dense(128, activation='relu', input_dim = train_reduced_features.shape[1]),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(8, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_final.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics=['accuracy', Precision(), Recall()])\n",
    "\n",
    "# Train the model\n",
    "model_final.fit(train_reduced_features, \n",
    "          train_labels,\n",
    "          validation_data = (val_reduced_features, val_labels),\n",
    "          epochs = 50, \n",
    "          batch_size = 16, \n",
    "          verbose = 1\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd51a84-d7e8-4788-9d27-d81c8a459131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the predictions\n",
    "combined_features_test = np.hstack((svm_test_predictions, densenet_test_predictions))\n",
    "\n",
    "# Apply the same PCA transformation\n",
    "reduced_features_test = pca.transform(combined_features_test)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "results = model_final.evaluate(reduced_features_test, test_labels)\n",
    "print(f\"Test Loss: {results[0]}\")\n",
    "print(f\"Test Accuracy: {results[1]}\")\n",
    "print(f\"Test Precision: {results[2]}\")\n",
    "print(f\"Test Recall: {results[3]}\")\n",
    "F1 = 2 * ((results[2] * results[3]) / (results[2] + results[3] + tf.keras.backend.epsilon()))\n",
    "print(f\"Test F1-Score: {F1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9636a1-3484-4dcf-b5d8-eee2dee970cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Predict classes\n",
    "test_predictions = model_final.predict(reduced_features_test)\n",
    "test_pred_classes = np.argmax(test_predictions, axis=1)\n",
    "\n",
    "# Assuming test_labels are integer labels\n",
    "print(classification_report(test_labels, test_pred_classes))\n",
    "print(confusion_matrix(test_labels, test_pred_classes))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
